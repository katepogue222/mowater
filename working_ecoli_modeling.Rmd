---
title: "Modeling ecoli reduction"
author: "Kate Pogue"
date: "2023-06-22"
output: html_document
---

Loading Packages
```{r}
library(tidyverse)
library(Metrics)
library(tidyr)
library(dplyr)
library(olsrr)
library(openxlsx)
library(zoo)
library(leaps)
library(ggeffects)
library(glmnet)
library(gratia)
```

Loading Data
```{r}
all_eps_filled <- read_csv("data/all_eps_filled.csv")
all_eps_removed <- read_csv("data/all_eps_removed.csv")
```


Make a heatmap of the FILL method data
```{r}

#for filled 

cor_matrix <- cor(all_eps_filled[,-c(1, 77:79)])

heatmap(cor_matrix, 
        col = colorRampPalette(c("blue", "white", "red"))(100),  # Choose a color palette
        main = "Correlation Heatmap for All 'Filled' Variables",
        xlab = "",
        ylab = "")


correlations <- cor(all_eps_filled[, -c(1,2,77:79)], all_eps_filled[, 2])

#for removed
cor_matrix <- cor(all_eps_removed[,-c(1, 77:79)])

heatmap(cor_matrix, 
        col = colorRampPalette(c("blue", "white", "red"))(100),  # Choose a color palette
        main = "Correlation Heatmap for All 'Removed' Variables",
        xlab = "",
        ylab = "")


```


NEW test/train split for levels
```{r}
result <- train_test_rolling_split(all_eps_filled, y_col = "log_ecoli", x_drop = c("timestamp", "log_reduction", "system", "day_of_week", "month"), train_frac = 0.7, scale = "StandardScaler")

train <- result$train
test <- result$test
```




Try lasso regression for FILL levels
```{r}

#make testing and training dataset
#set.seed(61)
#split_per <- .8 #Setting Trainging Size
#n <- sample(x = nrow(all_eps_filled), size = ceiling(split_per*nrow(all_eps_filled))) #Getting Training observation indexes
#train <- all_eps_filled[n,] #Creating Training data
#test <- all_eps_filled[-n,] #Creating Testing data



#make dummy variables for month and weekday (not for system bc it will not be fed into the model)
#month_dummy <- model.matrix(~ month - 1, data = all_eps_filled)
#day_of_week_dummy <- model.matrix(~ day_of_week - 1, data = all_eps_filled)


#extablish variables and make model
x <- cbind(as.matrix(all_eps_filled[, -c(1:3, 77:79)]))
y <- all_eps_filled$log_ecoli # outcome variable
cv_model <- cv.glmnet(x, y, alpha = 1, family = "gaussian")


# Plot with title
plot(cv_model)


#Extract Optimal Lambda
opt_lambda <- cv_model$lambda.1se
lasso_model <- glmnet(x, y, alpha = 1, family = "gaussian", lambda = opt_lambda)
plot(lasso_model)
coef(lasso_model)


#Predict on the test set

#levels_all <- levels(train$month)  # Get all levels of `month` variable from the training data
#month_dummy <- model.matrix(~ month - 1, data = test, levels = levels_all)
#day_of_week_dummy <- model.matrix(~ day_of_week - 1, data = test)

x_test <- cbind(as.matrix(train[, -c(1:3, 77:79)]))
y_pred <- predict(lasso_model, newx = x_test)




#visualize

# Predict the response variable using the Lasso model
predicted_values <- predict(lasso_model, newx = x)
time <- all_eps_filled$timestamp

# Plot the observed values over time
plot(time, y, pch = 16, col = "#42CEB3", xlab = "Time", ylab = "Log Transformed E. coli Reduction (EC/100mL)")

# Add the Lasso predictions over time
points(time, predicted_values, pch = 16, col = "#710C66")

# Add a title to the plot
title("Observed and Predicted E. coli Concentration over Time")

# Add legend
legend("bottomleft", legend = c("Observed", "Predicted"), pch = 16, col = c("#42CEB3", "#710C66"))




# Create color vectors based on "system" variable
obs_color <- ifelse(system == 2, "#42CEB3", "#D95E4F")
pred_color <- ifelse(system == 2, "#E4A458", "#710C66")

# Plot the observed values over time with color differentiation based on "system"
plot(time, y, pch = 16, col = obs_color, xlab = "Time", ylab = "Log Transformed E. coli Reduction (EC/100mL)")

# Add the Lasso predictions over time with color differentiation based on "system"
points(time, predicted_values, pch = 16, col = pred_color)

# Add a title to the plot
title("Observed and Predicted E. coli Concentration over Time")

# Move the legend to the outside
legend("bottomleft", legend = c("Observed Pilot", "Observed Control", "Predicted Pilot", "Predicted Control"),
       pch = 16, col = c("#42CEB3", "#D95E4F", "#E4A458", "#710C66"), bty = "n")


```




Evaluate Lasso for FILL
```{r}
lasso_model <- glmnet(x, y, alpha = 0, family = "gaussian", lambda = opt_lambda)

x_test <- cbind(as.matrix(train[, -c(1:3, 77:79)]))
y_pred <- predict(lasso_model, newx = x_test)


# Predict the response variable using the Lasso model
predicted_values <- predict(lasso_model, newx = x)

# Calculate residuals
residuals <- y - predicted_values

# Scatter plot of residuals over time
plot(all_eps_filled$timestamp, residuals, type = "p", xlab = "Time", ylab = "Residuals")

# Sort data by timestamp
sorted_data <- train[order(train$timestamp), ]
sorted_timestamp <- sorted_data$timestamp
sorted_residuals <- residuals[order(train$timestamp)]

# Line plot of residuals over time
plot(sorted_timestamp, sorted_residuals, type = "l", xlab = "Time", ylab = "Residuals")


# Scatter plot of residuals over predicted values
plot(predicted_values, residuals, xlab = "Predicted E. Coli Values", ylab = "Residuals")

# Create a histogram of residuals
hist(residuals, breaks = "FD", col = "lightblue", xlab = "Residuals", ylab = "Frequency", main = "Histogram of Residuals")

# Create a density plot of residuals
plot(density(residuals), main = "Density Plot of Residuals", xlab = "Residuals", ylab = "Density")

# Calculate RMSE
rmse <- sqrt(mean(residuals^2))

# Calculate MSE
mse <- mean(residuals^2)

# Calculate R-squared
y_mean <- mean(y)  # Mean of the actual target variable
ss_residual <- sum(residuals^2)  # Sum of squared residuals
ss_total <- sum((y - y_mean)^2)  # Total sum of squares
rsquared <- 1 - (ss_residual / ss_total)

# Print the RMSE, MSE, and R-squared
cat("RMSE:", rmse, "\n")
cat("MSE:", mse, "\n")
cat("R-squared:", rsquared, "\n")

```



Get Bootstrapped confidence intervals for FILL levels
```{r}
set.seed(107)  

x <- cbind(as.matrix(all_eps_filled[, -c(1:3, 77:79)]))
y <- all_eps_filled$log_ecoli
n.bootstrap <- 10000
n_features <- ncol(x) + 1  # Adding 1 for the intercept term
beta.results <- matrix(NA, nrow = n_features, ncol = n.bootstrap)

for (i in 1:n.bootstrap) {
  row.indices <- 1:174
  bootstrap.indices <- sample(row.indices, replace = TRUE)
  bootstrap.data <- x[bootstrap.indices, ]
  bootstrap.y <- y[bootstrap.indices]
  lasso_model <- glmnet(bootstrap.data, bootstrap.y, alpha = 1, family = "gaussian", lambda = 0.03457257205)
  beta.results[, i] <- as.vector(coef(lasso_model))
}

v <- c("intercept", colnames(all_eps_filled[, -c(1:3, 77:79)]))


# Calculate the quantiles for each column of beta.results
quantiles <- apply(beta.results, 1, quantile, probs = c(0.05, 0.95))

# Create the interval dataframe
interval <- data.frame(t(quantiles))

interval <- cbind(v, interval)

```



Get Bootstrapped confidence intervals for FILL REMOVAL
```{r}
set.seed(100)  

x <- cbind(as.matrix(all_eps_filled[, -c(1:3, 77:79)]))
y <- all_eps_filled$log_reduction
n.bootstrap <- 10000
n_features <- ncol(x) + 1  # Adding 1 for the intercept term
beta.results <- matrix(NA, nrow = n_features, ncol = n.bootstrap)

for (i in 1:n.bootstrap) {
  row.indices <- 1:174
  bootstrap.indices <- sample(row.indices, replace = TRUE)
  bootstrap.data <- x[bootstrap.indices, ]
  bootstrap.y <- y[bootstrap.indices]
  lasso_model <- glmnet(bootstrap.data, bootstrap.y, alpha = 1, family = "gaussian", lambda = 0.01)
  beta.results[, i] <- as.vector(coef(lasso_model))
}

v <- c("intercept", colnames(all_eps_filled[, -c(1:3, 77:79)]))


# Calculate the quantiles for each column of beta.results
quantiles <- apply(beta.results, 1, quantile, probs = c(0.05, 0.95))

# Create the interval dataframe
interval <- data.frame(t(quantiles))

interval <- cbind(v, interval)

```




Try lasso regression for FILL REMOVAL
```{r}

#make testing and training dataset
set.seed(61)
split_per <- .7 #Setting Trainging Size

n <- sample(x = nrow(all_eps_filled), size = ceiling(split_per*nrow(all_eps_filled))) #Getting Training observation indexes

train <- all_eps_filled[n,] #Creating Training data
test <- all_eps_filled[-n,] #Creating Testing data


#make dummy variables for month and weekday (not for system bc it will not be fed into the model)
month_dummy <- model.matrix(~ month - 1, data = train)
day_of_week_dummy <- model.matrix(~ day_of_week - 1, data = train)

#attach dummies to train
x <- cbind(as.matrix(train[, -c(1:3, 77:79)]))
y <- train$log_reduction #outcome variable
cv_model <- cv.glmnet(x, y, alpha = 1, family = "gaussian")
plot(cv_model)



#Extract Optimal Lambda
opt_lambda <- cv_model$lambda.1se
lasso_model <- glmnet(x, y, alpha = 1, family = "gaussian", lambda = opt_lambda)
plot(lasso_model)
coef(lasso_model)

#Predict on the test set

#levels_all <- levels(train$month)  # Get all levels of `month` variable from the training data
#month_dummy <- model.matrix(~ month - 1, data = test, levels = levels_all)
#day_of_week_dummy <- model.matrix(~ day_of_week - 1, data = test)

x_test <- cbind(as.matrix(train[, -c(1:3, 77:79)]))
y_pred <- predict(lasso_model, newx = x_test)


# Example: Calculate mean squared error
mse <- mean((train$log_reduction - y_pred)^2)


#visualize
time <- train$timestamp
floc <- train$Floc_percent
# Predict the response variable using the Lasso model
predicted_values <- predict(lasso_model, newx = x)


# Plot the observed values over time
plot(time, y, pch = 16, col = "#42CEB3", xlab = "Time", ylab = "Log Transformed E. coli Reduction (EC/100mL)")

# Add the Lasso predictions over time
points(time, predicted_values, pch = 16, col = "#710C66")

# Add a title to the plot
title("Observed and Predicted E. coli Reductions over Time")

# Add legend
legend("bottomleft", legend = c("Observed", "Predicted"), pch = 16, col = c("#42CEB3", "#710C66"))




# Create color vectors based on "system" variable
obs_color <- ifelse(system == 2, "#42CEB3", "#D95E4F")
pred_color <- ifelse(system == 2, "#E4A458", "#710C66")

# Plot the observed values over time with color differentiation based on "system"
plot(time, y, pch = 16, col = obs_color, xlab = "Time", ylab = "Log Transformed E. coli Reduction (EC/100mL)")

# Add the Lasso predictions over time with color differentiation based on "system"
points(time, predicted_values, pch = 16, col = pred_color)

# Add a title to the plot
title("Observed and Predicted E. coli Reductions over Time")

# Move the legend to the outside
legend("bottomleft", legend = c("Observed Pilot", "Observed Control", "Predicted Pilot", "Predicted Control"),
       pch = 16, col = c("#42CEB3", "#D95E4F", "#E4A458", "#710C66"), bty = "n")


```









































Try lasso regression for REMOVE levels
```{r}

#make testing and training dataset
set.seed(61)
split_per <- .8 #Setting Trainging Size

n <- sample(x = nrow(all_eps_removed), size = ceiling(split_per*nrow(all_eps_removed))) #Getting Training observation indexes

train <- all_eps_removed[n,] #Creating Training data
test <- all_eps_removed[-n,] #Creating Testing data

#make dummy variables for month and weekday (not for system bc it will not be fed into the model)
month_dummy <- model.matrix(~ month - 1, data = train)
day_of_week_dummy <- model.matrix(~ day_of_week - 1, data = train)

#attach dummies to train
x <- cbind(as.matrix(train[, -c(1:3, 77:79)]))
y <- train$log_ecoli #outcome variable
cv_model <- cv.glmnet(x, y, alpha = 1, family = "gaussian")
plot(cv_model)



#Extract Optimal Lambda
opt_lambda <- cv_model$lambda.1se
lasso_model <- glmnet(x, y, alpha = 1, family = "gaussian", lambda = opt_lambda)
plot(lasso_model)
coef(lasso_model)

#Predict on the test set

#levels_all <- levels(train$month)  # Get all levels of `month` variable from the training data
#month_dummy <- model.matrix(~ month - 1, data = test, levels = levels_all)
#day_of_week_dummy <- model.matrix(~ day_of_week - 1, data = test)

x_test <- cbind(as.matrix(train[, -c(1:3, 77:79)]))
y_pred <- predict(lasso_model, newx = x_test)


# Example: Calculate mean squared error
mse <- mean((test$log_ecoli - y_pred)^2)


#visualize
time <- train$timestamp
floc <- train$Floc_percent
# Predict the response variable using the Lasso model
predicted_values <- predict(lasso_model, newx = x)


# Plot the observed values over time
plot(time, y, pch = 16, col = "#42CEB3", xlab = "Time", ylab = "Log Transformed E. coli Reduction (EC/100mL)")

# Add the Lasso predictions over time
points(time, predicted_values, pch = 16, col = "#710C66")

# Add a title to the plot
title("Observed and Predicted E. coli Concentration over Time")

# Add legend
legend("bottomleft", legend = c("Observed", "Predicted"), pch = 16, col = c("#42CEB3", "#710C66"))




# Create color vectors based on "system" variable
obs_color <- ifelse(system == 2, "#42CEB3", "#D95E4F")
pred_color <- ifelse(system == 2, "#E4A458", "#710C66")

# Plot the observed values over time with color differentiation based on "system"
plot(time, y, pch = 16, col = obs_color, xlab = "Time", ylab = "Log Transformed E. coli Reduction (EC/100mL)")

# Add the Lasso predictions over time with color differentiation based on "system"
points(time, predicted_values, pch = 16, col = pred_color)

# Add a title to the plot
title("Observed and Predicted E. coli Concentration over Time")

# Move the legend to the outside
legend("bottomleft", legend = c("Observed Pilot", "Observed Control", "Predicted Pilot", "Predicted Control"),
       pch = 16, col = c("#42CEB3", "#D95E4F", "#E4A458", "#710C66"), bty = "n")





# Predict the response variable using the Lasso model
predicted_values <- predict(lasso_model, newx = x)

# Calculate R-squared
rss <- sum((y - predicted_values)^2)  # Residual sum of squares
tss <- sum((y - mean(y))^2)  # Total sum of squares
r_squared <- 1 - (rss / tss)  # R-squared

# Print the R-squared value
cat("R-squared:", r_squared, "\n")

```




Try lasso regression for REMOVE REMOVAL
```{r}

#make testing and training dataset
set.seed(61)
split_per <- .7 #Setting Trainging Size

n <- sample(x = nrow(all_eps_removed), size = ceiling(split_per*nrow(all_eps_removed))) #Getting Training observation indexes

train <- all_eps_removed[n,] #Creating Training data
test <- all_eps_removed[-n,] #Creating Testing data


#make dummy variables for month and weekday (not for system bc it will not be fed into the model)
month_dummy <- model.matrix(~ month - 1, data = train)
day_of_week_dummy <- model.matrix(~ day_of_week - 1, data = train)

#attach dummies to train
x <- cbind(as.matrix(train[, -c(1:3, 77:79)]))
y <- train$log_reduction #outcome variable
cv_model <- cv.glmnet(x, y, alpha = 1, family = "gaussian")
plot(cv_model)



#Extract Optimal Lambda
opt_lambda <- cv_model$lambda.1se
lasso_model <- glmnet(x, y, alpha = 1, family = "gaussian", lambda = opt_lambda)
plot(lasso_model)
coef(lasso_model)

#Predict on the test set

#levels_all <- levels(train$month)  # Get all levels of `month` variable from the training data
#month_dummy <- model.matrix(~ month - 1, data = test, levels = levels_all)
#day_of_week_dummy <- model.matrix(~ day_of_week - 1, data = test)

x_test <- cbind(as.matrix(train[, -c(1:3, 77:79)]))
y_pred <- predict(lasso_model, newx = x_test)


# Example: Calculate mean squared error
mse <- mean((train$log_reduction - y_pred)^2)


#visualize
time <- train$timestamp
floc <- train$Floc_percent
# Predict the response variable using the Lasso model
predicted_values <- predict(lasso_model, newx = x)


# Plot the observed values over time
plot(time, y, pch = 16, col = "#42CEB3", xlab = "Time", ylab = "Log Transformed E. coli Reduction (EC/100mL)")

# Add the Lasso predictions over time
points(time, predicted_values, pch = 16, col = "#710C66")

# Add a title to the plot
title("Observed and Predicted E. coli Reductions over Time")

# Add legend
legend("bottomleft", legend = c("Observed", "Predicted"), pch = 16, col = c("#42CEB3", "#710C66"))




# Create color vectors based on "system" variable
obs_color <- ifelse(system == 2, "#42CEB3", "#D95E4F")
pred_color <- ifelse(system == 2, "#E4A458", "#710C66")

# Plot the observed values over time with color differentiation based on "system"
plot(time, y, pch = 16, col = obs_color, xlab = "Time", ylab = "Log Transformed E. coli Reduction (EC/100mL)")

# Add the Lasso predictions over time with color differentiation based on "system"
points(time, predicted_values, pch = 16, col = pred_color)

# Add a title to the plot
title("Observed and Predicted E. coli Reductions over Time")

# Move the legend to the outside
legend("bottomleft", legend = c("Observed Pilot", "Observed Control", "Predicted Pilot", "Predicted Control"),
       pch = 16, col = c("#42CEB3", "#D95E4F", "#E4A458", "#710C66"), bty = "n")


```



Fit and test model for FILL method
```{r}
#view(colnames(all_eps_filled))

#make testing and training dataset
set.seed(1)
split_per <- .8 #Setting Trainging Size

n <- sample(x = nrow(all_eps_filled), size = ceiling(split_per*nrow(all_eps_filled))) #Getting Training observation indexes

train <- all_eps_filled[n,] #Creating Training data
test <- all_eps_filled[-n,] #Creating Testing data

#fitting full model
mod_full <- lm(data = train[,-1], log_reduction~.)

#using ols_step_all to get all predictor variables possibilities
subset <- ols_step_best_subset(mod_full) |> as_tibble()


#attaching all_eps_filled to environment
suppressMessages(attach(train))

#Allocating tibble for looping
subset_all <- tibble("predictors" = rep(NA, times = nrow(subset)),
             "rmse" = rep(NA, times = nrow(subset)),
             "adjr" = rep(NA, times = nrow(subset)))

#Creating list of all model possibilities
l <- lapply(1:nrow(subset), function(i){
  
  predictors <- subset$predictors[i] |> str_split(" ") #getting predictor variables
  
  #Creating tibble with log_ecoli as first column
  tb <- tibble("log_ecoli" = train$log_ecoli)

  #Adding each variable to the tibble
  for (j in 1:length(predictors[[1]])) {
  
  tb[,1+j] <- get(predictors[[1]][j])
  colnames(tb) <- c("log_ecoli", predictors[[1]])
  }

  mod <- lm(data = tb, log_ecoli ~.) #Fitting model with variables selected through the loop
  
  pred <- predict(mod, newdata = test) #Computing the model predictions
  rmse <- rmse(test$log_ecoli, pred) #Computing the model RMSE
  
  # Allocating Model Selection criteria to the list
  list("predictors" = subset$predictors[i] |> unlist(), 
       "rmse" = rmse(test$log_ecoli, pred), 
       "adjr" = summary(mod)$adj.r.squared
       )
}
)

#Extracting the information from the list into a data frame 
#by looping through the list so we can easily compare models
for (i in 1:nrow(subset)) {
  
  subset_all[i,1] <- l[[i]][1]
  subset_all[i,2] <- l[[i]][2]
  subset_all[i,3] <- l[[i]][3]
}

detach(train) #detaching all_eps_filled from environment 
rm(i, l)


##Look at results

subset_all |> 
   arrange(desc(adjr))


#Creating Table of top models combined with our model of interest
table <- subset_all |> 
   arrange(desc(adjr)) |> 
  rbind( # Combining the models into tabular form
    subset_all 
  ) |> 
  #Renaming and formatting the variables for table presentation
  select(predictors, rmse, adjr) |> 
  transmute("Predictor Variables" = str_replace_all(predictors, pattern = " ", replacement = ", "),
            "RMSE" = round(rmse, 3)|> format(format = "d", big.mark = ","),
            "Adj.R2" = round(adjr, 3)) |> 
  as.matrix(nrow = 4, ncol = 4)


table |> 
  knitr::kable(caption = "E. coli Regression Models for Filled dataset") #Creating table

```




Fit and test model for REMOVE method
```{r}
#make testing and training dataset
set.seed(1)
split_per <- .8 #Setting Trainging Size

n <- sample(x = nrow(all_eps_removed), size = ceiling(split_per*nrow(all_eps_removed))) #Getting Training observation indexes

train <- all_eps_removed[n,] #Creating Training data
test <- all_eps_removed[-n,] #Creating Testing data

#fitting full model
mod_full <- lm(data = train[, c(2:15)], log_ecoli~.)

#using ols_step_all to get all predictor variables possibilities
subset <- ols_step_all_possible(mod_full) |> as_tibble()


#Allocating tibble for looping
subset_all <- tibble("predictors" = rep(NA, times = nrow(subset)),
             "rmse" = rep(NA, times = nrow(subset)),
             "adjr" = rep(NA, times = nrow(subset)))

#Creating list of all model possibilities
l <- lapply(1:nrow(subset), function(i){
  
  predictors <- subset$predictors[i] |> str_split(" ") #getting predictor variables
  
  #Creating tibble with log_ecoli as first column
  tb <- tibble("log_ecoli" = train$log_ecoli)

  #Adding each variable to the tibble
  for (j in 1:length(predictors[[1]])) {
  
  tb[,1+j] <- get(predictors[[1]][j])
  colnames(tb) <- c("log_ecoli", predictors[[1]])
  }

  mod <- lm(data = tb, log_ecoli ~.) #Fitting model with variables selected through the loop
  
  pred <- predict(mod, newdata = test) #Computing the model predictions
  rmse <- rmse(test$log_ecoli, pred) #Computing the model RMSE
  
  # Allocating Model Selection criteria to the list
  list("predictors" = subset$predictors[i] |> unlist(), 
       "rmse" = rmse(test$log_ecoli, pred), 
       "adjr" = summary(mod)$adj.r.squared
       )
}
)

#Extracting the information from the list into a data frame 
#by looping through the list so we can easily compare models
for (i in 1:nrow(subset)) {
  
  subset_all[i,1] <- l[[i]][1]
  subset_all[i,2] <- l[[i]][2]
  subset_all[i,3] <- l[[i]][3]
}



##Look at results

subset_all |> 
  arrange(rmse)

mean(test$log_ecoli)

#Creating Table of top 3 models combined with our model of interest
table <- subset_all |> 
  arrange(rmse) |> 
  rbind( # Combining the models into tabular form
    subset_all 
  ) |> 
  #Renaming and formatting the variables for table presentation
  select(predictors, rmse, adjr) |> 
  transmute("Predictor Variables" = str_replace_all(predictors, pattern = " ", replacement = ", "),
            "RMSE" = round(rmse, 1)|> format(format = "d", big.mark = ","),
            "Adj.R2" = round(adjr, 3)) |> 
  as.matrix(nrow = 4, ncol = 4)


table |> 
  knitr::kable(caption = "E. coli Regression Models for Removed dataset") #Creating table


```


